{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/juzykaggle/nn-sequential-optimization?scriptVersionId=139406102\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 7\nnp.random.seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filling missing values whith median\nfor i in train.columns:\n    if train[i].isna().sum()>0:\n        train[i] = train[i].fillna(train[i].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get dummies for categorial feature\ntrain['EJ'] = pd.get_dummies(train['EJ'], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['Id'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['Class'], axis=1)\ny = train.Class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile base model by Sequential","metadata":{}},{"cell_type":"code","source":"# baseline model with 56 neurons (such as number of features)\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(56, input_dim=56, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model \nestimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, y, cv=kfold)\nprint(results.mean()*100, results.std()*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neural network models are specifically designed to provide consistent input measurements at scale as well as at distribution.\nAn effective scheme for preparing data for tabular data when building neural network models is standardization. \nHere, the data is scaled such that the mean for each attribute is 0 and the standard deviation is 1.\nWe use the scikit-learn standard to process our dataset using the StandardScaler for the class.\nInstead of performing standardization on the entire dataset, \nit is good practice to train the training data standardization procedure during a cross-validation pass \nand use the trained standardization to prepare an \"invisible\" test set. \nThis makes standardization a model preparation step in the cross-validation process \nand prevents the algorithm from using knowledge of \"invisible\" data during evaluation, \nknowledge that can be transferred from the data preparation scheme, such as a sharper distribution.","metadata":{}},{"cell_type":"code","source":"# evaluate baseline model with standardized dataset\nnp.random.seed(seed)\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, y, cv=kfold)\nprint(results.mean()*100, results.std()*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a small but very pleasant rise in medium accuracy.\nIn next experiment, we take our base model with 56 neurons in the hidden layer and halve it to 28. \nThis will put pressure on the network during training to choose the most important input structure for the model.","metadata":{}},{"cell_type":"code","source":"# smaller model\ndef create_smaller_model():\n    model = Sequential()\n    model.add(Dense(28, input_dim=56, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_smaller_model, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, y, cv=kfold)\nprint(\"Standartized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model worked a little more closely\nLet's check bigger model whith adding another layer","metadata":{}},{"cell_type":"code","source":"# bigger model\ndef create_bigger_model():\n    model = Sequential()\n    model.add(Dense(56, input_dim=56, kernel_initializer='normal', activation='relu'))\n    # adding layer\n    model.add(Dense(26, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standartize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_bigger_model, epochs=100, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, y, cv=kfold)\nprint(\"Standartized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Increasing the layers in the network leads to a decrease in the score","metadata":{}},{"cell_type":"markdown","source":"Tuning the hyperparameters","metadata":{}},{"cell_type":"code","source":"# tuning of batch_size and epochs\nparam_batch_epochs = {'batch_size': range(5, 20, 5), 'epochs': range(50, 200, 50)}\nmodel_batch_epochs = KerasClassifier(build_fn=create_smaller_model, verbose=0)\ngrid_batch_epochs = GridSearchCV(model_batch_epochs, param_grid=param_batch_epochs)\nbatch_epochs_res = grid_batch_epochs.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_epochs_res.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_epochs_res.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning weights initialization\ndef create_smaller_model_for_tune(init):\n    model = Sequential()\n    model.add(Dense(28, input_dim=56, kernel_initializer=init, activation='relu'))\n    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ninit = ['uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nparam_weights = dict(init=init)\nmodel_weights = KerasClassifier(build_fn=create_smaller_model_for_tune, batch_size = 15,epochs = 150, verbose=0)\ngrid_weights = GridSearchCV(model_weights, param_grid=param_weights)\nweights_res = grid_weights.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_res.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_res.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuning Dropout Regularization\ndef create_smaller_model_for_tune_dropout(dropout_rate, weight_constraint):\n    model = Sequential()\n    model.add(Dense(28, input_dim=56, kernel_initializer='uniform', activation='relu', kernel_constraint=tf.keras.constraints.max_norm(weight_constraint)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nweight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\nparam_dropout = dict(dropout_rate = dropout_rate, weight_constraint=weight_constraint)\n\nmodel_dropout = KerasClassifier(build_fn=create_smaller_model_for_tune_dropout, verbose=0)\ngrid_dropout = GridSearchCV(model_dropout, param_grid=param_dropout)\ndropout_res = grid_dropout.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropout_res.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropout_res.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final model\ndef create_final_model():\n    model = Sequential()\n    model.add(Dense(28, input_dim=56, kernel_initializer='uniform', activation='relu'), tf.keras.constraints.max_norm(4))\n    model.add(Dropout(0.6))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_final_model, epochs=150, batch_size=15, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, y, cv=kfold)\nprint(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}